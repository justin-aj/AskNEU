# Model Evaluation

This directory contains tools for evaluating model-generated answers against ground truth using RAGAS (Retrieval Augmented Generation Assessment).

## Overview

The evaluation system takes an Excel file with questions, ground truth answers, and model-generated answers as input, processes them through RAGAS metrics, and produces a new Excel file with evaluation scores.


## Input Format

The system requires an input file named `question.xlsx` with the following columns:
- `Question`: The query or question
- `Ground Truth`: The reference answer (ground truth)
- `Model Generated`: The answer generated by the model being evaluated

## Usage

1. Place your `question.xlsx` file in this directory
2. Run the evaluation script:
   ```bash
   python ragas_evaluation.py
   ```
3. The script will generate `evaluation_results_final.xlsx` containing the original data plus evaluation scores

## Evaluation Metrics

The system evaluates model responses on 5 RAGAS metrics, which may include:
- Relevance: Does the response directly address the query? Does it stay on-topic and relate well to the question?
- Accuracy: Is the information provided factually correct? Does the answer align with the provided ground truth?
- Completeness: Does the response cover all necessary aspects of the question? Are there any important details left out?
- Clarity: Is the response easy to understand? Is the language clear, and is the meaning conveyed effectively?
- Conciseness: Is the response free from unnecessary information? Does it avoid verbosity while answering thoroughly?

## CI/CD Integration

The system is set up with CI/CD automation to run evaluations whenever a new `question.xlsx` file is uploaded to the directory:

1. When a new `question.xlsx` file is detected in the directory
2. The `ragas_evaluation.py` script is automatically triggered
3. Evaluation is performed and results are saved to `evaluation_results_final.xlsx`
