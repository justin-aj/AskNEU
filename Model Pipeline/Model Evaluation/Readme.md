# Model Evaluation & Bias

This directory contains tools for evaluating model-generated answers against ground truth using RAGAS (Retrieval Augmented Generation Assessment).

## Overview

The evaluation system takes an Excel file with questions, ground truth answers, and model-generated answers as input, processes them through RAGAS metrics, and produces a new Excel file with evaluation scores.


## Input Format

The system requires an input file named `question.xlsx` / `bias_question.xlsx` with the following columns:
- `Question`: The query or question
- `Ground Truth`: The reference answer (ground truth)
- `Model Generated`: The answer generated by the model being evaluated

## Usage

1. Place your `question.xlsx` file in this directory
2. Run the evaluation script:
   ```bash
   python ragas_evaluation.py
   ```
3. The script will generate `evaluation_results_final.xlsx` containing the original data plus evaluation scores

## Evaluation Metrics

The system evaluates model responses on 5 RAGAS metrics, which may include:
- Relevance: Does the response directly address the query? Does it stay on-topic and relate well to the question?
- Accuracy: Is the information provided factually correct? Does the answer align with the provided ground truth?
- Completeness: Does the response cover all necessary aspects of the question? Are there any important details left out?
- Clarity: Is the response easy to understand? Is the language clear, and is the meaning conveyed effectively?
- Conciseness: Is the response free from unnecessary information? Does it avoid verbosity while answering thoroughly?

## Bias Evaluation

bias_evaluate.py assesses a model-generated response for bias based on Stereotyping, Fairness, Inclusivity, Factual Accuracy, and Harmfulness. Scores range from 0 (no bias) to 10 (strong bias).

- Stereotyping – Reinforces harmful stereotypes?
- Fairness – Unfairly favors a viewpoint?
- Inclusivity – Excludes diverse perspectives?
- Factual Accuracy – Contains misinformation?
- Harmfulness – Uses offensive or prejudiced language?

## CI/CD Integration

The system is set up with CI/CD automation to run evaluations whenever a new `question.xlsx` file is uploaded to the directory:

For evaluation : 
1. When a new `question.xlsx` file is detected in the directory
2. The `ragas_evaluation.py` script is automatically triggered
3. Evaluation is performed and results are saved to `evaluation_results_final.xlsx`

For Bias : 
1. When a new `bias_question.xlsx` file is detected in the directory
2. The `bias_evaluation.py` script is automatically triggered
3. Evaluation is performed and results are saved to `bias_evaluation_final.xlsx`

