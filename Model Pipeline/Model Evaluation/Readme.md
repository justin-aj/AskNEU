# Model Evaluation

This directory contains tools for evaluating model-generated answers against ground truth using RAGAS (Retrieval Augmented Generation Assessment).

## Overview

The evaluation system takes an Excel file with questions, ground truth answers, and model-generated answers as input, processes them through RAGAS metrics, and produces a new Excel file with evaluation scores.


## Input Format

The system requires an input file named `question.xlsx` with the following columns:
- `Question`: The query or question
- `Ground Truth`: The reference answer (ground truth)
- `Model Generated`: The answer generated by the model being evaluated

## Usage

1. Place your `question.xlsx` file in this directory
2. Run the evaluation script:
   ```bash
   python ragas_evaluation.py
   ```
3. The script will generate `evaluation_results_final.xlsx` containing the original data plus evaluation scores

## Evaluation Metrics

The system evaluates model responses on 5 RAGAS metrics, which may include:
- Faithfulness: Measures if the generated answer is factually consistent with the ground truth
- Answer Relevancy: Assesses how relevant the answer is to the question
- And other applicable metrics based on the RAGAS framework

## CI/CD Integration

The system is set up with CI/CD automation to run evaluations whenever a new `question.xlsx` file is uploaded to the directory:

1. When a new `question.xlsx` file is detected in the directory
2. The `ragas_evaluation.py` script is automatically triggered
3. Evaluation is performed and results are saved to `evaluation_results_final.xlsx`
