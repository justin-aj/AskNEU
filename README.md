# AskNEU - MLOps Project [Team - 18]

 AskNEU is a conversational Retrieval-Augmented Generation (RAG) system designed to transform how users interact with Northeastern University's vast repository of information. AskNEU delivers accurate, context-aware answers to user queries in real time by integrating advanced AI language models with targeted data retrieval techniques. Whether you're a prospective student exploring academic programs, a current student navigating campus resources, or a faculty member seeking policy details, AskNEU serves as your intelligent assistant.
 
Traditional information retrieval systems often require users to sift through multiple web pages or documents to find specific answers. AskNEU eliminates this by combining the precision of search engines with the natural language understanding of GPT-based models. When a user asks a question, the system first retrieves the most relevant data from Northeastern's official website and then generates a concise, conversational response tailored to the command. This approach ensures that users receive accurate real-time information.

The project is driven by the goal of providing accurate information for the Northeastern community. AskNEU bridges the gap between complex institutional data and user-friendly communication. Our vision is to create a system that not only answers questions but also anticipates user needs, providing proactive suggestions and insights. This project represents a significant step forward in making institutional knowledge more accessible, intuitive, and engaging for everyone connected to Northeastern University.

---

# Web Scraping and Embedding Pipeline

This repository contains an automated data pipeline built with Apache Airflow to scrape web data, store it in Google Cloud Storage (GCS), version it with DVC, and process it into embeddings stored in Pinecone. The pipeline is designed for modularity, reproducibility, and scalability, adhering to Python PEP 8 standards.

## Table of Contents
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Running the Pipeline](#running-the-pipeline)
- [Code Structure](#code-structure)
- [Reproducibility](#reproducibility)
- [Error Handling & Logging](#error-handling--logging)
- [Expected Outcome](#expected-outcome)
- [Contributing](#contributing)

---

## Project Structure

```
/Project Repo
|-- Data-Pipeline/          # Root directory for pipeline-related files
|   |-- dags/               # Airflow DAG files
|   |   |-- scraping_pipeline_gcs.py       # DAG for scraping and GCS upload
|   |   |-- gcs_to_pinecone_embedding.py   # DAG for embedding and Pinecone upsert
|   |-- data/               # Local storage for scraped data
|   |-- scripts/            # Python scripts (e.g., scraping logic)
|   |   |-- Scrape_script.py
|   |   |-- requirements.txt
|   |-- tests/              # Unit tests for scripts
|   |   |-- test_scraper.py
|   |-- logs/               # Log files generated by Airflow
|   |-- dvc.yaml            # DVC configuration for data versioning
|-- docker-compose.yml      # Docker Compose configuration for Airflow
|-- README.md               # Project documentation
```

---

## Prerequisites

- **Docker**: For containerized Airflow setup
- **Docker Compose**: To manage Airflow services
- **Google Cloud SDK**: For GCS access
- **DVC**: For data versioning
- **Pinecone Account**: For embedding storage

---

## Setup Instructions

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-username/your-repo.git
   cd your-repo/Data-Pipeline
   ```

2. **Set Up Docker Compose**:
   - Ensure `docker-compose.yml` is present in the root directory (or create one based on Airflow’s official example).
   - Start Airflow services:
     ```bash
     docker-compose up -d
     ```
   - This launches the Airflow webserver, scheduler, and database.

3. **Access Airflow UI**:
   - Once Airflow is up, open `http://localhost:8080` in your browser.
   - Log in with:
     - **Username**: `airflow`
     - **Password**: `airflow`

4. **Configure GCP Connection**:
   - In the Airflow UI, go to `Admin > Connections`.
   - Add a new connection:
     - **Conn Id**: `gcp_service_account`
     - **Conn Type**: `Google Cloud`
     - **Keyfile JSON**: Paste your GCS service account JSON (e.g., `{"type": "service_account", "project_id": "xxxx", ...}`).
   - Save the connection.

5. **Set Airflow Variables**:
   - Go to `Admin > Variables` in the UI.
   - Add:
     - **Key**: `GCS_BUCKET_NAME`
     - **Value**: Your GCS bucket URL (e.g., `my-gcs-bucket`).
   - Save the variable.

6. **Install DVC** (if not in Docker image):
   - If your Docker image doesn’t include DVC, exec into the container:
     ```bash
     docker exec -it <airflow-container-name> bash
     pip install dvc[gcs]
     ```

7. **Set Up Pinecone**:
   - Update `utils/config_utils.py` with your Pinecone credentials:
     ```python
     "pinecone": {
         "api_key": "your-pinecone-api-key",
         "environment": "your-pinecone-environment",
         "index_name": "your-index-name",
         "namespace": "your-namespace"
     }
     ```

---

## Running the Pipeline

1. **Start Airflow**:
   - If not already running, use:
     ```bash
     docker-compose up -d
     ```
   - Verify services in the Airflow UI at `http://localhost:8080`.

2. **Enable DAGs**:
   - In the UI, toggle on:
     - `scraping_pipeline_gcs`: Scrapes data, validates it, versions with DVC, and uploads to GCS.
     - `gcs_to_pinecone_embedding`: Downloads from GCS, generates embeddings, and upserts to Pinecone.

3. **Monitor Execution**:
   - Check task status in the Airflow UI.
   - Logs are available in the `logs/` directory or via the UI.

---

## Code Structure

- **`dags/scraping_pipeline_gcs.py`**:
  - **Tasks**:
    - `install_requirements`: Installs script dependencies.
    - `run_unittests`: Runs unit tests on the scraper.
    - `extract_data`: Executes the scraping script.
    - `validate_data`: Ensures scraped files exist.
    - `dvc_push`: Versions data with DVC and pushes to GCS.
    - `alert_failure`: Alerts on any task failure.
  - **Flow**:
    ```python
    (install_requirements >> run_unittests >> extract_data >> validate_data >> dvc_push)
    [run_unittests, extract_data, validate_data, dvc_push] >> alert_failure
    ```

- **`dags/gcs_to_pinecone_embedding.py`**:
  - Downloads files from GCS in batches, generates embeddings, and upserts to Pinecone.
  - Tasks: List files, download batches, process embeddings, upsert, cleanup.

- **`scripts/`**:
  - `Scrape_script.py`: Web scraping logic.
  - `requirements.txt`: Script dependencies.

- **`tests/`**:
  - `test_scraper.py`: Unit tests for the scraper.

- **`utils/`**:
  - Helper functions for configuration, storage, embeddings, and Pinecone.

---

## Reproducibility

To replicate the pipeline:

1. **Clone and Set Up**:
   - Follow [Setup Instructions](#setup-instructions).

2. **Pull Versioned Data with DVC**:
   - Exec into the Airflow container:
     ```bash
     docker exec -it <airflow-container-name> bash
     ```
   - Initialize DVC and pull data:
     ```bash
     dvc init
     dvc remote add -d my_gcs gs://your-gcs-bucket-name
     dvc pull
     ```

3. **Run the Pipeline**:
   - Enable DAGs as described in [Running the Pipeline](#running-the-pipeline).

- **Data Versioning**: DVC tracks `data/` and syncs with GCS via `dvc.yaml`.
- **Dependencies**: Managed via `scripts/requirements.txt`.

---

## Error Handling & Logging

- **Error Handling**:
  - Scraping failures raise `AirflowFailException`.
  - GCS and Pinecone tasks handle exceptions gracefully.
  - Batch processing adjusts dynamically to file counts.

- **Logging**:
  - Logs are stored in `logs/` and accessible via the Airflow UI.
  - Task-specific logs (e.g., file uploads) aid troubleshooting.

---

## Expected Outcome

- Once all tasks in `scraping_pipeline_gcs` complete successfully:
  - Scraped `.txt` files appear in your GCS bucket under the `scraped_texts/` prefix.
- After `gcs_to_pinecone_embedding` runs:
  - Embeddings are upserted to Pinecone, verifiable via the Pinecone dashboard.


